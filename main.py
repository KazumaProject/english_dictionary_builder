# (0) è¿½åŠ ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from datasets import load_dataset, get_dataset_config_names
from collections import Counter
import numpy as np
from tqdm.auto import tqdm
import spacy
from itertools import islice
import re

# ----------------------------------------------------------------------
# â˜…â˜…â˜… è¨­å®šé …ç›® â˜…â˜…â˜…
N_GRAM_SIZE = 1
SCORE_TYPE   = "cost"

# GitHub Actions ã® 6h åˆ¶é™å†…ã§åã¾ã‚‹ã‚ˆã†ã«èª¿æ•´
MAX_SAMPLES_TO_PROCESS = 750_000     # Wikipedia ã¯ 1 è¨˜äº‹ãŒé•·ã„ã®ã§å°‘ã—æ¸›ã‚‰ã™

# â”€â”€ (NEW) æœ€æ–° Wikipedia è‹±èªãƒ€ãƒ³ãƒ—ã® config åã‚’è‡ªå‹•å–å¾— â”€â”€
all_cfgs         = get_dataset_config_names("wikimedia/wikipedia")
LATEST_WIKI_CFG  = max(c for c in all_cfgs if re.match(r"\d{8}\.en$", c))   # yyyyMMdd.en

DATASET_CONFIGS = [
    # æœ€æ–° Wikipedia è‹±èª
    {"name": "wikimedia/wikipedia", "config": LATEST_WIKI_CFG, "split": "train", "column": "text"},
    # å®šç•ªã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå°ã•ã‚ã§å‹•ä½œç¢ºèªã—ã‚„ã™ã„ï¼‰
    {"name": "wikitext", "config": "wikitext-103-v1", "split": "train", "column": "text"},
]

output_filename = f"{N_GRAM_SIZE}-grams_{SCORE_TYPE}_{LATEST_WIKI_CFG}_wikitext.txt"
# ----------------------------------------------------------------------

# (1) spaCy ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆé«˜é€ŸåŒ–ç”¨ã« parser/ner ç„¡åŠ¹åŒ–ï¼‰
print("Loading spaCy model...")
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
print("âœ… spaCy model loaded.")

# (2) ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã¨å˜èªãƒ¡ã‚¿
unigram_counts = Counter()
word_details   = {}

# (3) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é †ç•ªã«å‡¦ç†
for cfg in DATASET_CONFIGS:
    print(f"\nğŸ“š Processing {cfg['name']} ({cfg['config']}) â€¦")
    ds_stream = load_dataset(
        path   = cfg["name"],
        name   = cfg.get("config"),
        split  = cfg["split"],
        streaming = True,
        # Wikipedia ã¯ Apache-Beam ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹
        beam_runner = "DirectRunner",
        trust_remote_code = True,
    )

    # å¿…è¦ä»¶æ•°ã ã‘å–ã‚Šå‡ºã™
    ds_subset = islice(ds_stream, MAX_SAMPLES_TO_PROCESS)
    texts     = (row[cfg["column"]] for row in ds_subset)

    for doc in tqdm(nlp.pipe(texts, batch_size=500), total=MAX_SAMPLES_TO_PROCESS,
                    desc=f"spaCy â†” {cfg['name']}"):
        toks = [t.lower_ for t in doc if t.is_alpha and not t.is_stop]
        for t in toks:
            if t not in word_details or doc[toks.index(t)].pos_ == "PROPN":
                word_details[t] = {"original": t, "pos": doc[toks.index(t)].pos_}
        unigram_counts.update(toks)

print("\nâœ… ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é›†è¨ˆå®Œäº†")

# â”€â”€ ä»¥ä¸‹ (4)ã€œ(7) ã¯ã»ã¼ãã®ã¾ã¾ â”€â”€
# ...
